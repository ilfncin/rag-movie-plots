{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a83fd827",
   "metadata": {},
   "source": [
    "# **RAG Ingestion Pipeline**\n",
    "\n",
    "**Notebook Setup and Autoreload**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "023fd1d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook environment configured successfully!\n",
      "\n",
      "Project root: /home/ilfn/datascience/workspace/rag-movie-plots\n",
      "Added to sys.path:\n",
      "  - /home/ilfn/datascience/workspace/rag-movie-plots/src\n",
      "  - /home/ilfn/datascience/workspace/rag-movie-plots/src/backend\n",
      "PYTHONPATH: /home/ilfn/datascience/workspace/rag-movie-plots/src\n",
      "Current working directory: /home/ilfn/datascience/workspace/rag-movie-plots/notebooks\n"
     ]
    }
   ],
   "source": [
    "# --- Notebook setup and autoreload configuration ---\n",
    "# This cell runs the initial setup script and enables the autoreload extension.\n",
    "# The autoreload feature ensures that updates made to imported modules (e.g., in src/)\n",
    "# are automatically reloaded without restarting the kernel.\n",
    "%run notebook_setup.py\n",
    "\n",
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b293362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os, sys\n",
    "# print(\"CWD:\", os.getcwd())\n",
    "# print(\"PYTHONPATH:\", sys.path[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56087b1",
   "metadata": {},
   "source": [
    "**1. ETL — Data Cleaning and Structured Output Generation**\n",
    "\n",
    "This step runs the **ETL pipeline** responsible for preparing the raw dataset of movie plots for subsequent stages of the RAG system.\n",
    "\n",
    "The process includes **three key operations**:\n",
    "* `Data Cleaning`:\n",
    "    * The **raw CSV** file is loaded and cleaned using the `DataCleaner` class (`data_cleaner`), which removes invalid tokens, standardizes formats, and applies column-specific transformations.\n",
    "* `Structured Output Generation`:\n",
    "    * The **cleaned dataset** is then saved as a **new CSV file** through the `DataPipeline` class (`data_pipeline`).\n",
    "* `JSONL Creation`:\n",
    "    * Finally, the `JsonlWriter` class (`jsonl_writer`) converts the structured dataset into **JSON Lines (.jsonl)** format, separating the main text (`Plot`) from its **metadata** (e.g., title, genre, year, cast, director).\n",
    "\n",
    "In summary, this step transforms the raw movie dataset into a clean, structured, and RAG-ready format — enabling reliable chunking and semantic representation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f1314a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 1: ETL (Data Cleaning + JSONL Creation) ---\n",
    "# Loads the raw movie dataset, cleans invalid tokens using DataCleaner,\n",
    "# generates a structured CSV, and produces the docs.jsonl file for downstream steps.\n",
    "\n",
    "# Equivalent to running from the terminal:\n",
    "# PYTHONPATH=src uv run src/backend/main.py --step etl\n",
    "\n",
    "# import sys, runpy\n",
    "# sys.argv = [\"backend/main.py\", \"--step\", \"etl\"]\n",
    "# _ = runpy.run_module(\"main\", run_name=\"__main__\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a86ee95",
   "metadata": {},
   "source": [
    "_Expected output_:\n",
    "* `movies_clean.csv` (cleaned dataset)\n",
    "* `docs.jsonl` (RAG-ready JSON lines file)\n",
    "\n",
    "**2. Chunking — Text Splitting for Document Segmentation**\n",
    "\n",
    "This step splits the cleaned movie plot texts into smaller, context-preserving chunks suitable for embedding and retrieval.\n",
    "\n",
    "It performs **three major operations**:\n",
    "* `Chunk Loading`:\n",
    "    * The `.jsonl` file produced during ETL is loaded and parsed into a collection of documents containing `text` and `metadata`.\n",
    "* `Splitting Strategy Application`:\n",
    "    * The `ChunkingPipeline` applies the configured chunking strategy (character, token, or recursive), using parameters defined in `CHUNKING_CONFIG` to determine chunk size and overlap.\n",
    "    * This ensures contextual continuity while limiting token counts per segment.\n",
    "* `Chunk Output Serialization`:\n",
    "    * Each document’s text is split into multiple sub-documents, preserving metadata such as `Title`, `Release Year`, and a unique `id`.\n",
    "    * The resulting collection is stored in a new `.jsonl` file (`chunks.jsonl`) for embedding.\n",
    "\n",
    "This stage ensures that the RAG system operates over optimized text units — large enough to preserve context but small enough for efficient vector search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9cb1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 2: Chunking ---\n",
    "# Splits the text documents from docs.jsonl into smaller chunks using the configured strategy\n",
    "# (character, token, or recursive) for better retrieval and embedding performance.\n",
    "\n",
    "# Equivalent to running from the terminal:\n",
    "# PYTHONPATH=src uv run src/backend/main.py --step chunking\n",
    "\n",
    "# import sys, runpy\n",
    "# sys.argv = [\"backend/main.py\", \"--step\", \"chunking\"]\n",
    "# _ = runpy.run_module(\"main\", run_name=\"__main__\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5b182b",
   "metadata": {},
   "source": [
    "_Expected output_:\n",
    "* `chunks.jsonl` file containing segmented text chunks with metadata.\n",
    "\n",
    "**3. VectorStore — Embedding and Persistence**\n",
    "\n",
    "In this stage, the pre-chunked documents are transformed into vector representations and stored in a persistent vector database.\n",
    "\n",
    "The process includes the following steps:\n",
    "* `Chunk Loading`:\n",
    "    * The `chunks.jsonl` file is read and converted into `langchain_core.documents.Document` objects, combining each text segment with its corresponding metadata.\n",
    "* `Embedding Generation`:\n",
    "    * The `VectorStorePipeline` uses a Hugging Face embedding model (as defined in `EMBEDDING_CONFIG`) to generate dense semantic vectors for each chunk.\n",
    "* `Vector Storage`:\n",
    "    * These vectors are persisted into a local **ChromaDB** instance (`VECTORSTORE_CONFIG[\"persist_dir\"]`), enabling fast similarity-based retrieval for the RAG retriever.\n",
    "\n",
    "This step converts textual knowledge into an efficient, queryable vector representation — the foundation for semantic search and contextual question answering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ca1f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 3: VectorStore Creation ---\n",
    "# Loads the text chunks, generates embeddings using the configured\n",
    "# Hugging Face model, and stores them in a persistent ChromaDB database.\n",
    "\n",
    "# Equivalent to running from the terminal:\n",
    "# PYTHONPATH=src uv run src/backend/main.py --step vectorstore\n",
    "\n",
    "# import sys, runpy\n",
    "# sys.argv = [\"backend/main.py\", \"--step\", \"vectorstore\"]\n",
    "# _ = runpy.run_module(\"main\", run_name=\"__main__\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7d0dba",
   "metadata": {},
   "source": [
    "**4. Full Pipeline Execution**\n",
    "Running the **Full Ingestion Pipeline** (`pipeline.run_full()`) executes all three stages sequentially:\n",
    "1. `ETL`: cleans and structures the dataset\n",
    "2. `Chunking`: splits the text into retrieval-friendly chunks\n",
    "3. `VectorStore`: embeds and persists the chunks in ChromaDB\n",
    "\n",
    "The full execution ensures reproducibility across environments — from local notebooks to production pipelines — enabling seamless data ingestion for Retrieval-Augmented Generation systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c367f7b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ilfn/datascience/workspace/rag-movie-plots/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting full RAG preprocessing phase of the RAG workflow pipeline...\n",
      "\n",
      "Loaded raw dataset with 34886 rows from /home/ilfn/datascience/workspace/rag-movie-plots/data/raw/wiki_movie_plots_deduped.csv\n",
      "Cleaned CSV saved to /home/ilfn/datascience/workspace/rag-movie-plots/data/processed/v20251111/movies_clean.csv\n",
      "JSONL file created at /home/ilfn/datascience/workspace/rag-movie-plots/data/processed/v20251111/docs.jsonl\n",
      "Data processing completed successfully.\n",
      "\n",
      "ETL completed.\n",
      "\n",
      "Saving 245859 chunks to /home/ilfn/datascience/workspace/rag-movie-plots/data/processed/v20251111/chunks.jsonl\n",
      "\n",
      "Chunking completed.\n",
      "\n",
      "Reading chunks from /home/ilfn/datascience/workspace/rag-movie-plots/data/processed/v20251111/chunks.jsonl...\n",
      "Preparing 245859 documents for vector storage...\n",
      "Generating embeddings with model: sentence-transformers/all-MiniLM-L6-v2\n",
      "Persisting ChromaDB at: /home/ilfn/datascience/workspace/rag-movie-plots/db/chroma\n",
      "\n",
      "VectorStore creation completed.\n",
      "\n",
      "Full pipeline finished successfully.\n",
      "\n",
      "[full] completed in 01:45:57\n"
     ]
    }
   ],
   "source": [
    "# --- Optional: Full Ingestion Execution ---\n",
    "# Runs all three stages (ETL -> Chunking -> VectorStore) sequentially.\n",
    "# Equivalent to running from the terminal:\n",
    "# PYTHONPATH=src uv run src/backend/main.py --step full\n",
    "\n",
    "import sys, runpy\n",
    "\n",
    "sys.argv = [\"backend/main.py\", \"--step\", \"full\"]\n",
    "_ = runpy.run_module(\"main\", run_name=\"__main__\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7722501",
   "metadata": {},
   "source": [
    "_Expected output_:\n",
    "* Complete ingestion process executed end-to-end.\n",
    "* All artifacts generated under `data/processed/vYYYYMMDD/`.\n",
    "\n",
    "## **Notebook Summary**\n",
    "\n",
    "**This notebook allows you to**:\n",
    "* Run and debug each ingestion step individually.\n",
    "* Inspect intermediate outputs (`movies_clean.csv`, `docs.jsonl`, `chunks.jsonl`).\n",
    "* Ensure the full ingestion pipeline works consistently across environments (notebook, CLI, production)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-movie-plots",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
